{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://laura.alessandretti.com/comsocsci2023/assignments.html) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Feb 28th at 23:55. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Assignment, Assignment 1)_. \n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository (if you don't have a shared Git repository, it's fine. Remember to do it next time)\n",
    "* group members' contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Using web-scraping to gather data\n",
    "\n",
    "Gather the list of researchers that have joined the most important scientific conference in Computational Social Science in 2019. \n",
    "\n",
    "You can find the programmes of the 2019 edition at the links below:  \n",
    ">Oral presentations: https://2019.ic2s2.org/oral-presentations/    \n",
    "> Poster presentations: https://2019.ic2s2.org/posters/    \n",
    "\n",
    "1. Inspect the HTML of the pages above and use web-scraping to get the set of participants in 2019. Share your code and add comments to guide us through it.\n",
    "2. How many unique researchers you got in 2019?\n",
    "3. Explain one or two decisions you took during the web-scraping exercise, for 2019 or any other year. Why did you take this choice? How might your decision impact the final number of authors?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking out the oral presentations. The process for this is cumbersome, and is explained in detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "LINK_2019 = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "r_2019 = requests.get(LINK_2019) \n",
    "r_2019.content\n",
    "soup_2019 = BeautifulSoup(r_2019.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the side has been loaded in, and it is time to extract the relevant data. From inspecting the page manually, we find that we should search for all the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><strong>1B Culture and Art</strong> – Thursday July 18, 11:00 – 12:30 <br/><em>Chair: Milan Janosov</em><br/>11:00 – 11:15 – Kangsan Lee, Jaehyuk Park, Yong-Yeol Ahn. Valuing Art: Professional vs. Algorithm<br/>11:15 – 11:30 – Milan Janosov, Federico Battiston, Gerardo Iñiguez, Federico Musciotto. Success and mentorship in electronic music<br/>11:30 – 11:45 – Sophie Cho, Michael Mauskapf. Unpacking the Ecological Dynamics of Cultural Products: Evidence from Popular Music<br/>11:45 – 12:00 – Susumu Nagayama, Hitoshi Mitsuhashi. Expandable and Extendable Root Concepts<br/>12:00 – 12:15 – Ramona Roller, Frank Schweitzer. Network regression reveals factors driving the letter communication of 16th century reformators<br/>12:15 – 12:30 – No Presentation</p>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = soup_2019.find_all('p')\n",
    "step0 = data[4]\n",
    "step0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to work some magic to extract the names. First, we can split the above on the word ```'Chair'```, which yields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p><strong>1B Culture and Art</strong> – Thursday July 18, 11:00 – 12:30 <br/><em>',\n",
       " 'Milan Janosov</em><br/>11:00 – 11:15 – Kangsan Lee, Jaehyuk Park, Yong-Yeol Ahn. Valuing Art: Professional vs. Algorithm<br/>11:15 – 11:30 – Milan Janosov, Federico Battiston, Gerardo Iñiguez, Federico Musciotto. Success and mentorship in electronic music<br/>11:30 – 11:45 – Sophie Cho, Michael Mauskapf. Unpacking the Ecological Dynamics of Cultural Products: Evidence from Popular Music<br/>11:45 – 12:00 – Susumu Nagayama, Hitoshi Mitsuhashi. Expandable and Extendable Root Concepts<br/>12:00 – 12:15 – Ramona Roller, Frank Schweitzer. Network regression reveals factors driving the letter communication of 16th century reformators<br/>12:15 – 12:30 – No Presentation</p>']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = str(step0)\n",
    "step2 = step1.split('Chair: ')\n",
    "step2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the part after the split, i.e ```step2[-1]```, so this becomes step3. Also, we split on ```'<br/>'```, which yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kangsan Lee', 'Jaehyuk Park', 'Yong-Yeol Ahn']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = step2[-1]\n",
    "step4 = step3.split('<br/>')\n",
    "step5 = step4[1:] # Ignore the Chair name\n",
    "step6 = step5[0].split('– ') # Split on the dash to remove the time\n",
    "step7 = step6[-1].split(', ') # Split on the comma to seperate the authors\n",
    "step8 = step7[-1]\n",
    "step9 = step8.split('.')[0]\n",
    "\n",
    "authors = []\n",
    "authors.extend(step7[:-1])\n",
    "authors.extend([step9])\n",
    "authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this is only an example on one paragraph. However, we can write a function to do all of this at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraperv1(url: str) -> list:\n",
    "    r = requests.get(url) \n",
    "    soup = BeautifulSoup(r.content)\n",
    "    data = soup.find_all('p')[3:] # Ignore the first 3 entries\n",
    "\n",
    "    authors = []\n",
    "\n",
    "    for entry in data:\n",
    "        step1 = str(entry)\n",
    "        if '<strong>' not in step1: # Check if the entry is a presentation\n",
    "            continue\n",
    "        \n",
    "        step2 = step1.split('Chair: ')\n",
    "        step3 = step2[-1]\n",
    "        step4 = step3.split('<br/>')\n",
    "        presentaions = step4[1:] # Ignore the Chair name (step5)\n",
    "        for presentation in presentaions:\n",
    "            step6 = presentation.split('– ') # Split on the dash\n",
    "            step7 = step6[-1].split(', ') # Split on the comma\n",
    "            step8 = step7[-1]\n",
    "            step9 = step8.split('.')[0]\n",
    "            authors.extend(step7[:-1])\n",
    "            authors.extend([step9])\n",
    "\n",
    "    \n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jieyu Ding', 'Qiusi Sun', 'Jingwen Zhang', 'Mahmoudreza Babaei', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna Gummadi', 'Alexandre Bovet', 'Hernan Makse', 'Ziv Epstein', 'Mohsen Mosleh', 'Antonio Arechar', 'Gordon Pennycook', 'David Rand', 'Sunandan Chakraborty', 'Tarunima Prabhakar', 'Joyojeet Pal –', 'Dissecting Fake News: Understanding the Dynamics of False Information spread in India'] \n",
      " Number of authors 698\n"
     ]
    }
   ],
   "source": [
    "raw_authors = scraperv1(LINK_2019)\n",
    "print(raw_authors[:20],'\\n',f\"Number of authors {len(raw_authors)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the list of authors above, it is evident there are some flaws in our methods most noticably the titles appearing. We can try to filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jieyu Ding', 'Qiusi Sun', 'Jingwen Zhang', 'Mahmoudreza Babaei', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna Gummadi', 'Alexandre Bovet', 'Hernan Makse', 'Ziv Epstein', 'Mohsen Mosleh', 'Antonio Arechar', 'Gordon Pennycook', 'David Rand', 'Sunandan Chakraborty', 'Tarunima Prabhakar', 'Joyojeet Pal –', 'Gustavo Jota Resende'] \n",
      " Number of authors 667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lens = [len(author) for author in authors]\n",
    "mu = np.mean(lens)\n",
    "filtered_authors = [author for author in authors if len(author) < mu*2]\n",
    "print(filtered_authors[:20],'\\n',f\"Number of authors {len(filtered_authors)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are some errors with a dash being at the end of a name. We can simply fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jieyu Ding', 'Qiusi Sun', 'Jingwen Zhang', 'Mahmoudreza Babaei', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna Gummadi', 'Alexandre Bovet', 'Hernan Makse', 'Ziv Epstein', 'Mohsen Mosleh', 'Antonio Arechar', 'Gordon Pennycook', 'David Rand', 'Sunandan Chakraborty', 'Tarunima Prabhakar', 'Joyojeet Pal', 'Gustavo Jota Resende'] \n",
      " Number of authors 667\n"
     ]
    }
   ],
   "source": [
    "clean_authors = [name if name[-1] != '–' else name[:-2] for name in filtered_authors] # Remove the dash at the end of the name\n",
    "print(clean_authors[:20],'\\n',f\"Number of authors {len(clean_authors)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we take the set to remove potential duplicates, and save the authors to a df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors 549\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "authors_oral = list(set(clean_authors)) # Remove duplicates\n",
    "df = pd.DataFrame(authors_oral, columns=['Authors'])\n",
    "\n",
    "if not os.path.exists('author_data'):\n",
    "   os.makedirs('author_data')\n",
    "\n",
    "df.to_csv('author_data/authors_oral.csv', index=False)\n",
    "with open('author_data/authors_oral.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(f\"Number of authors {len(authors_oral)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we find a total of 549 authors from the 2019 oral prensentations. This is not guaranteed to only contain names, and there might still be flaws with the names themselves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will go through the poster presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraperv2(url: str) -> list:\n",
    "    authors = []\n",
    "    \n",
    "    r = requests.get(url) \n",
    "    soup = BeautifulSoup(r.content)\n",
    "    data = soup.find_all('li') # Get all the list items\n",
    "    texts = [line.text for line in data] # Get the text from the list items\n",
    "\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        temp.extend(line.split(\"\\xa0\\n\")) # Split on the non-breaking space character and new line character\n",
    "    \n",
    "    temp1 = temp[32:360] # Ignore the first 32 entries and the last 24 entries\n",
    "    temp2 = []\n",
    "    for line in temp1:\n",
    "        temp2.extend(line.split(\"\\xa0\")) # Split on the non-breaking space character\n",
    "\n",
    "    temp3 = []\n",
    "    for line in temp2: \n",
    "        temp3.extend(line.split(\"\\n\")) # Split on the new line character\n",
    "\n",
    "    names = temp3[::2] # Get every other entry\n",
    "\n",
    "    temp4 = []\n",
    "    for line in names: \n",
    "        temp4.extend(line.split(\",\")) # Split on the comma\n",
    "\n",
    "    temp5 = []\n",
    "    for line in temp4:\n",
    "        temp5.extend(line.split(\"and \")) # Split on the word \"and\"\n",
    "\n",
    "    temp6 = list(set(temp5)) # Remove duplicates\n",
    "    temp6.remove(\"\") # Remove empty strings\n",
    "\n",
    "    for line in temp6:\n",
    "        authors.append(line[1:]) if line[0] == \" \" else authors.append(line) # Remove the space at the beginning of the name\n",
    "\n",
    "    authors = list(set(authors)) # Remove duplicates\n",
    "    authors.remove(\"\") # Remove empty strings\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the function to get the authors from the 2019 poster presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find 475 authors.\n"
     ]
    }
   ],
   "source": [
    "LINK_POSTER = \"https://2019.ic2s2.org/posters/\"\n",
    "\n",
    "authors_poster = scraperv2(LINK_POSTER)\n",
    "df2 = pd.DataFrame(authors_poster, columns=['Authors'])\n",
    "df2.to_csv('author_data/authors_poster.csv', index=False)\n",
    "with open('author_data/authors_poster.pickle', 'wb') as f:\n",
    "    pickle.dump(df2, f)\n",
    "\n",
    "print(f\"We find {len(authors_poster)} authors.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two lists of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we find 949 unique authors in 2019.\n"
     ]
    }
   ],
   "source": [
    "total_authors = final_authors + authors_poster\n",
    "total_authors.remove('No presentation</p>')\n",
    "total_authors = list(set(total_authors))\n",
    "print(f\"In total, we find {len(total_authors)} unique authors in 2019.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we find a total of 949 authors from the oral and poster presentations fro 2019. This of course included many decisions for filtering, and many of these decisions might not have been optimal. However, the layout of the two websites made it very tedious to gather only the information you want without also removing some in the process. A great example is when we filter the names from oral presentations after the length to remove presentation titles. This might have an unwanted effect of also removing some long names, and might not even have removed all the non-names. Unfortunately, that is a trade-off, that must be made."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2020 and 2021\n",
    "\n",
    "We also gathered the authors from 2020 and 2021. This code will not be commented but results will be shown.\n",
    "We start with 2020 which included a Google Sheets document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td class=\"s1\">Tutorial</td>, <td class=\"s1\">Tutorial: Identifying social media manipulation with OSoMe tools</td>, <td class=\"s1\">Diogo Pacheco, Kaicheng Yang, Pik-mai Hui</td>, <td class=\"s1\">July 17, 2020, 02:00:00 PM</td>, <td class=\"s1\">July 17, 2020, 05:00:00 PM</td>]\n"
     ]
    }
   ],
   "source": [
    "LINK = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTX9_1Xftn7D-nSI8X9b7tafr_Z0kAbphKdfZ8qUSU9p-syXNsGPdhHl5ZyTnKKL-T6dCEJqtsrn3wy/pubhtml/sheet?headers=false&gid=181378784\"\n",
    "r = requests.get(LINK) \n",
    "soup = BeautifulSoup(r.content) \n",
    "\n",
    "class_name =  \"waffle\"\n",
    "table = soup.find(\"table\",{\"class\": class_name})\n",
    "rows = table.find_all(\"tr\") \n",
    "names = rows[3].find_all(\"td\", {\"class\": \"s1\"})\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = []\n",
    "boole = []\n",
    "for row in rows[2:]:\n",
    "    stuff = row.find_all(\"td\", {\"class\": \"s1\"})\n",
    "    if stuff[2].text != '':\n",
    "        all_names.append(stuff[2].text.split(', '))\n",
    "\n",
    "final_names = []\n",
    "for name in all_names:\n",
    "    final_names.extend(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find 990 authors in 2020.\n"
     ]
    }
   ],
   "source": [
    "authors_2020 = list(set(final_names))\n",
    "\n",
    "print(f\"We find {len(authors_2020)} authors in 2020.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(final_names, columns=['Authors'])\n",
    "df3.to_csv('author_data/authors_2020.csv', index=False)\n",
    "with open('author_data/authors_2020.pickle', 'wb') as f:\n",
    "    pickle.dump(df3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK = \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "r = requests.get(LINK) \n",
    "soup = BeautifulSoup(r.content) \n",
    "\n",
    "class_name =  \"index\"\n",
    "table = soup.find(\"table\",{\"class\": class_name})\n",
    "temp = table.find_all(\"td\", {\"class\": \"name\"})\n",
    "names = []\n",
    "for name in temp:\n",
    "    both = name.text.split(', ')\n",
    "    names.append(both[1]+\" \"+both[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find 692 authors in 2021.\n"
     ]
    }
   ],
   "source": [
    "authors_2021 = list(set(names))\n",
    "print(f\"We find {len(authors_2021)} authors in 2021.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(names_2021, columns=['Authors'])\n",
    "df4.to_csv('author_data/authors_2021.csv', index=False)\n",
    "with open('author_data/authors_2021.pickle', 'wb') as f:\n",
    "    pickle.dump(df4, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally collect all authors in one dataframe. This will be the basis of the part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we find 2261 unique authors across all years (2019, 2020, 2021).\n"
     ]
    }
   ],
   "source": [
    "authors = authors_oral + authors_poster + authors_2020 + authors_2021\n",
    "unique_authors = list(set(authors))\n",
    "print(f\"In total, we find {len(unique_authors)} unique authors across all years (2019, 2020, 2021).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = pd.DataFrame(unique_authors, columns=['Authors'])\n",
    "df_authors.to_csv('authors.csv', index=False)\n",
    "with open('authors.pickle', 'wb') as f:\n",
    "    pickle.dump(df_authors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting data from the Semantic Scholar API\n",
    "\n",
    "> * Consider the list of author ids you have found in Week 2, Part 3, first excercise. For each author, use the Academic Graph API to find:\n",
    ">    - their _aliases_\n",
    ">    - their _name_\n",
    ">    - their _papers_, where for each paper we want to retain: \n",
    ">        -  _title_ \n",
    ">        -  _abstract_ \n",
    ">        -  the _year_ of publication\n",
    ">        -  the _externalIds_ (this is because there are universal identifiers for scientific works called DOI that we can use across platforms)\n",
    ">        -  _s2FieldsOfStudy_ the fields of study\n",
    ">        - _citationCount_ the number of times that this paper was cited    \n",
    "> * Create three dataframe to store the data you have collected. \n",
    ">    \n",
    ">    - **Author dataset:** in the author dataset, one raw is one unique author, and each row contains the following information: \n",
    ">        - *authorId*: (str) the id of the author\n",
    ">        - *name*: (str) the name of the author\n",
    ">        - *aliases*: (list) the aliases of the author\n",
    ">        - *citationCount*: (int) the total number of citations received by an author\n",
    ">        - *field*: (str) the _s2FieldsOfStudy_ that occurs most times across an author's papers (you should first obtain the *category* for each _s2FieldsOfStudy_)\n",
    ">    - **Paper dataset:** in the paper dataset, one row is one unique paper, and each row contains the following information:\n",
    ">        - *paperId*: (str) the id of the paper\n",
    ">        - *title*: (str) the title of the paper\n",
    ">        - *year*: (int) the year of publication\n",
    ">        - *externalId.DOI:* (str) the DOI of the paper\n",
    ">        - *citationCount*: (int) the number of citations\n",
    ">        - *fields*: (list) the fields included in the paper (you should first obtain the *category* for each _s2FieldsOfStudy_)\n",
    ">        - *authorIds:* (list) this is a list of *author Ids*, including all the authors of this paper that are in our author dataset\n",
    ">    - **Paper abstract dataset:** in the paper abstract dataset, one row is one unique paper, and each row contains the following information: \n",
    ">        - *paperId*: (str) the id of the paper\n",
    ">        - *abstract*: (str) the abstract of the paper    \n",
    ">  (Note: we keep the abstract separate to keep the size of files more manageable)\n",
    "\n",
    "\n",
    "1. Share the number of authors you will use as starting point in this exercises. Add a comment clarifying how many IC2S2 editions you included and if the collaborators were included or not.\n",
    "2. Share the code you have used to solve the exercise above.\n",
    "3. How long is your final _Author_ dataframe? How long is your final _Paper_ dataframe? \n",
    "\n",
    "(**Note**: If you did not manage to get all the years or all the authors' collaborators, you can still follow the exercise. Just remember to clarify your starting point.)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to 1.\n",
    "To address the first question, after collecting all the unique authors in Part 1 of this assignment, we had to find their collaborators. This came out to a total of 105674 unique author IDs. However, there were some trouble in this data collection process, and therefore the authors from the 2019 oral presentations were not included."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to 2.\n",
    "To address the second question, the code can be seen below. \n",
    "NB! This code WILL NOT BE EXECUTED considering the fact that it will likely run for ~24 hours before completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = {\n",
    "    \"ids\": [],\n",
    "    \"names\": [],\n",
    "    \"aliases\": [],\n",
    "    \"citationCount\": [],\n",
    "    \"field\": []\n",
    "    }\n",
    "\n",
    "paper_data = {\n",
    "    \"paperId\": [],\n",
    "    \"title\": [],\n",
    "    \"year\": [],\n",
    "    \"DOI\": [],\n",
    "    \"citationCount\": [],\n",
    "    \"fields\": [],\n",
    "    \"authorIds\": []\n",
    "}\n",
    "\n",
    "abstract_data = {\n",
    "    \"paperId\": [],\n",
    "    \"abstract\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>105674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2773799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       authorId\n",
       "count    105674\n",
       "unique   105674\n",
       "top     2773799\n",
       "freq          1"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors = pd.read_csv(\"all_authors.csv\")\n",
    "\n",
    "all_authors = all_authors.dropna()\n",
    "all_authors = all_authors.astype(int).astype(str)\n",
    "\n",
    "all_authors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/batch\"\n",
    "FIELDS = \"?fields=name,aliases,citationCount,papers.title,papers.abstract,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.authors\"\n",
    "\n",
    "my_url = BASE_URL + VERSION + RESOURCE + FIELDS\n",
    "\n",
    "\n",
    "def most_frequent(List: list):\n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0] \n",
    "\n",
    "def get_data(idx, url, status_count = 0):\n",
    "    ids = list(all_authors.iloc[idx:idx+10]['authorId'])\n",
    "    \n",
    "    r = requests.post(url, json={\"ids\": ids})\n",
    "\n",
    "    if r.status_code == 500 or r.status_code == 504:\n",
    "        if status_count == 2:\n",
    "            print(f'Failed on {idx} with status code {r.status_code}')\n",
    "            return None\n",
    "        return get_data(idx, url, status_count + 1) \n",
    "        \n",
    "    if r.status_code != 200:\n",
    "        time.sleep(30)\n",
    "        print(f\"Got status code: {r.status_code}. Trying Again!\")\n",
    "        get_data(idx, url, status_count)\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "def make_data(r):\n",
    "    if r is None or r == [None]: \n",
    "        return None\n",
    "    for author in r: \n",
    "        if author is None or author == [None]: \n",
    "            return None\n",
    "        author_data['ids'].append(author['authorId']) if author['authorId'] is not None else author_data['ids'].append(None)\n",
    "        author_data['names'].append(author['name']) if author['name'] is not None else author_data['names'].append(None)\n",
    "        author_data['aliases'].append(str(author['aliases'])) if author['aliases'] is not None else author_data['aliases'].append(None)\n",
    "        author_data['citationCount'].append(int(author['citationCount'])) if author['citationCount'] is not None else author_data['citationCount'].append(0)\n",
    "        temp_fields = []\n",
    "        for paper in author['papers']:\n",
    "            if 'DOI' in paper['externalIds'].keys():\n",
    "                paper_data[\"paperId\"].append(paper['paperId']) if paper['paperId'] is not None else paper_data[\"paperId\"].append(None)\n",
    "                paper_data[\"title\"].append(paper['title']) if paper['title'] is not None else paper_data[\"title\"].append(None)\n",
    "                paper_data[\"year\"].append(int(paper['year'])) if paper['year'] is not None else paper_data[\"year\"].append(None)\n",
    "                paper_data[\"citationCount\"].append(int(paper['citationCount'])) if paper['citationCount'] is not None else paper_data[\"citationCount\"].append(0)\n",
    "                paper_data[\"fields\"].append(str([field['category'] for field in paper['s2FieldsOfStudy']])) if paper['s2FieldsOfStudy'] is not None else paper_data[\"fields\"].append(None)\n",
    "                temp_fields.extend([field['category'] for field in paper['s2FieldsOfStudy']]) if paper['s2FieldsOfStudy'] is not None else None\n",
    "                aut = paper['authors'] if paper['authors'] is not None else None\n",
    "                paper_data[\"authorIds\"].append([author['authorId'] for author in aut]) if aut is not None else paper_data[\"authorIds\"].append(None)\n",
    "                paper_data[\"DOI\"].append(paper['externalIds']['DOI']) if paper['externalIds']['DOI'] is not None else paper_data[\"DOI\"].append(None)\n",
    "                abstract_data['paperId'].append(paper['paperId']) if paper['paperId'] is not None else abstract_data['paperId'].append(None)\n",
    "                abstract_data[\"abstract\"].append(paper['abstract']) if paper['abstract'] is not None else abstract_data[\"abstract\"].append(None)\n",
    "        author_data['field'].append(most_frequent(temp_fields)) if len(temp_fields) > 0 else author_data['field'].append(None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for idx in range(0, len(all_authors), 10):\n",
    "    make_data(get_data(idx, my_url, 0))\n",
    "    if idx % 100 == 0:\n",
    "        author_df = pd.DataFrame(author_data, columns=['ids', 'names', 'aliases', 'citationCount', 'field'])\n",
    "        paper_df = pd.DataFrame(paper_data, columns=['paperId', 'title', 'year', 'DOI', 'citationCount', 'fields', 'authorIds'])\n",
    "        abstract_df = pd.DataFrame(abstract_data, columns=['paperId', 'abstract'])\n",
    "        author_df.to_csv(\"author_data.csv\", index=False)\n",
    "        paper_df.to_csv(\"paper_data.csv\", index=False)\n",
    "        abstract_df.to_csv(\"abstract_data.csv\", index=False)\n",
    "        print(f\"Saved at {idx}!\")\n",
    "\"\"\"\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete code used for collecting the abstract-, author and paper data set. We can, however, load these in, and find the sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>names</th>\n",
       "      <th>aliases</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2773799</td>\n",
       "      <td>A. Filisetti</td>\n",
       "      <td>['A Filisetti', 'Alessandro Filisetti']</td>\n",
       "      <td>631</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1399712184</td>\n",
       "      <td>S. Cohen-Boulakia</td>\n",
       "      <td>['S Cohen-boulakia', 'Sarah Cohen‐boulakia', '...</td>\n",
       "      <td>757</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50197963</td>\n",
       "      <td>J. Ludwig</td>\n",
       "      <td>['J. Ludwig', 'Jens Ludwig']</td>\n",
       "      <td>11446</td>\n",
       "      <td>Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3530609</td>\n",
       "      <td>L. Dutra</td>\n",
       "      <td>['Lívia Macedo Dutra', 'Lívia M. Dutra', 'Lívi...</td>\n",
       "      <td>477</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84242025</td>\n",
       "      <td>Annelies E. M. van Vianen</td>\n",
       "      <td>['Annelies Elizabeth Maria Van Vianen', 'Annel...</td>\n",
       "      <td>2035</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>1765906</td>\n",
       "      <td>C. Wright</td>\n",
       "      <td>['Cameron D. Wright', 'C Wright', 'Cameron D W...</td>\n",
       "      <td>16400</td>\n",
       "      <td>Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39423</th>\n",
       "      <td>8247954</td>\n",
       "      <td>Jung‐Ah Shin</td>\n",
       "      <td>['Jung A Shin', 'Jung A. Shin', 'Jung Ah Shin'...</td>\n",
       "      <td>1114</td>\n",
       "      <td>Agricultural And Food Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39424</th>\n",
       "      <td>1393904172</td>\n",
       "      <td>Drew Blaisdell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39425</th>\n",
       "      <td>120711397</td>\n",
       "      <td>N. Raval</td>\n",
       "      <td>['Niyati Raval']</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39426</th>\n",
       "      <td>2080924918</td>\n",
       "      <td>Chris Chambers</td>\n",
       "      <td>['Christopher D. Chambers']</td>\n",
       "      <td>2</td>\n",
       "      <td>Economics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39427 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ids                      names  \\\n",
       "0         2773799               A. Filisetti   \n",
       "1      1399712184          S. Cohen-Boulakia   \n",
       "2        50197963                  J. Ludwig   \n",
       "3         3530609                   L. Dutra   \n",
       "4        84242025  Annelies E. M. van Vianen   \n",
       "...           ...                        ...   \n",
       "39422     1765906                  C. Wright   \n",
       "39423     8247954               Jung‐Ah Shin   \n",
       "39424  1393904172             Drew Blaisdell   \n",
       "39425   120711397                   N. Raval   \n",
       "39426  2080924918             Chris Chambers   \n",
       "\n",
       "                                                 aliases  citationCount  \\\n",
       "0                ['A Filisetti', 'Alessandro Filisetti']            631   \n",
       "1      ['S Cohen-boulakia', 'Sarah Cohen‐boulakia', '...            757   \n",
       "2                           ['J. Ludwig', 'Jens Ludwig']          11446   \n",
       "3      ['Lívia Macedo Dutra', 'Lívia M. Dutra', 'Lívi...            477   \n",
       "4      ['Annelies Elizabeth Maria Van Vianen', 'Annel...           2035   \n",
       "...                                                  ...            ...   \n",
       "39422  ['Cameron D. Wright', 'C Wright', 'Cameron D W...          16400   \n",
       "39423  ['Jung A Shin', 'Jung A. Shin', 'Jung Ah Shin'...           1114   \n",
       "39424                                                NaN             45   \n",
       "39425                                   ['Niyati Raval']              3   \n",
       "39426                        ['Christopher D. Chambers']              2   \n",
       "\n",
       "                                field  \n",
       "0                             Biology  \n",
       "1                    Computer Science  \n",
       "2                           Economics  \n",
       "3                           Chemistry  \n",
       "4                          Psychology  \n",
       "...                               ...  \n",
       "39422                        Medicine  \n",
       "39423  Agricultural And Food Sciences  \n",
       "39424                             NaN  \n",
       "39425                             NaN  \n",
       "39426                       Economics  \n",
       "\n",
       "[39427 rows x 5 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_data = pd.read_csv(\"author_data.csv\")\n",
    "author_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final author dataframe has 39427 entries. Note that this has not been cleaned for any duplicates that may have appeared nor have entries with NaN values been removed. Ideally, this dataframe should have just as many entries as the number of authors (i.e. 105674) but it was prematurely stopped after having run for ~7 ours over night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>DOI</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fields</th>\n",
       "      <th>authorIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>825f85375bba977cd3ad78ac1ba22c7fae5609fb</td>\n",
       "      <td>Reference-Grade Genome and Large Linear Plasmi...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>10.1128/spectrum.02434-21</td>\n",
       "      <td>1</td>\n",
       "      <td>['Biology', 'Engineering']</td>\n",
       "      <td>['7635770', '49271298', '2773799', '5179706', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ada32be518f395002d90c52befabcd8735128192</td>\n",
       "      <td>Synthetic biology approaches to actinomycete s...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.1093/femsle/fnab060</td>\n",
       "      <td>2</td>\n",
       "      <td>['Engineering', 'Biology']</td>\n",
       "      <td>['2661384', '8646979', '144778908', '51935399'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>765919117cdcc0c736502d0c08f3d3b985e017f8</td>\n",
       "      <td>Dynamical Criticality: Overview and Open Quest...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>10.1007/s11424-017-6117-5</td>\n",
       "      <td>0</td>\n",
       "      <td>['Psychology']</td>\n",
       "      <td>['1763293', '145479337', '2773799', '143851134']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0d783a10ec4f9293ffdf4bddf4c63f4e2b54b8b1</td>\n",
       "      <td>Beyond Networks: Search for Relevant Subsets i...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10.1007/978-3-319-24391-7_12</td>\n",
       "      <td>1</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>['1763293', '145479337', '2773799', '143851134']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25cb12204aeb78762dc00eab20e7c24927f96ea0</td>\n",
       "      <td>Dynamical regimes in non-ergodic random Boolea...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10.1007/s11047-016-9552-7</td>\n",
       "      <td>0</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>['145479337', '2221493', '35063515', '1763293'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372484</th>\n",
       "      <td>3d013d2db7615c688f2d02be30f544038c3b4a39</td>\n",
       "      <td>Monday, 27 August 2012</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.1093/EURHEARTJ/EHS282</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>['1397812736', '1399678807', '143698504', '205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372485</th>\n",
       "      <td>4fe4267decc3a1c9f3f8d5183e8ecf767c3f9325</td>\n",
       "      <td>Wednesday, 29 August 2012</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.1093/EURHEARTJ/EHS284</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['2960501', '2509309', '1702163', '1406005831'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372486</th>\n",
       "      <td>2df72f572ab17cd8ea3c4b40bb9e6a8cd965345a</td>\n",
       "      <td>Redefine statistical significance</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>10.1038/s41562-017-0189-z</td>\n",
       "      <td>0</td>\n",
       "      <td>['Economics']</td>\n",
       "      <td>[None, '39201543', '2205017432', '2862527', '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372487</th>\n",
       "      <td>5fd9466a17ff615b8ad302f91d1178dfa16c6c14</td>\n",
       "      <td>Suggestions to Advance Your Mission: An Open L...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>10.31234/osf.io/39ugj</td>\n",
       "      <td>2</td>\n",
       "      <td>['Medicine']</td>\n",
       "      <td>['2505272', '49115533', '103097236', '20809249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372488</th>\n",
       "      <td>75fdf6a4d993bd5a0ffd8491aab851df741f7107</td>\n",
       "      <td>Open letter to the Society for Neuroscience</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>10.15200/WINN.140865.54468</td>\n",
       "      <td>0</td>\n",
       "      <td>['Art']</td>\n",
       "      <td>['3149509', '1401864541', '2620049', '13900229...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2372489 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paperId  \\\n",
       "0        825f85375bba977cd3ad78ac1ba22c7fae5609fb   \n",
       "1        ada32be518f395002d90c52befabcd8735128192   \n",
       "2        765919117cdcc0c736502d0c08f3d3b985e017f8   \n",
       "3        0d783a10ec4f9293ffdf4bddf4c63f4e2b54b8b1   \n",
       "4        25cb12204aeb78762dc00eab20e7c24927f96ea0   \n",
       "...                                           ...   \n",
       "2372484  3d013d2db7615c688f2d02be30f544038c3b4a39   \n",
       "2372485  4fe4267decc3a1c9f3f8d5183e8ecf767c3f9325   \n",
       "2372486  2df72f572ab17cd8ea3c4b40bb9e6a8cd965345a   \n",
       "2372487  5fd9466a17ff615b8ad302f91d1178dfa16c6c14   \n",
       "2372488  75fdf6a4d993bd5a0ffd8491aab851df741f7107   \n",
       "\n",
       "                                                     title    year  \\\n",
       "0        Reference-Grade Genome and Large Linear Plasmi...  2022.0   \n",
       "1        Synthetic biology approaches to actinomycete s...  2021.0   \n",
       "2        Dynamical Criticality: Overview and Open Quest...  2017.0   \n",
       "3        Beyond Networks: Search for Relevant Subsets i...  2016.0   \n",
       "4        Dynamical regimes in non-ergodic random Boolea...  2016.0   \n",
       "...                                                    ...     ...   \n",
       "2372484                             Monday, 27 August 2012  2012.0   \n",
       "2372485                          Wednesday, 29 August 2012  2012.0   \n",
       "2372486                  Redefine statistical significance  2017.0   \n",
       "2372487  Suggestions to Advance Your Mission: An Open L...  2017.0   \n",
       "2372488        Open letter to the Society for Neuroscience  2014.0   \n",
       "\n",
       "                                  DOI  citationCount  \\\n",
       "0           10.1128/spectrum.02434-21              1   \n",
       "1              10.1093/femsle/fnab060              2   \n",
       "2           10.1007/s11424-017-6117-5              0   \n",
       "3        10.1007/978-3-319-24391-7_12              1   \n",
       "4           10.1007/s11047-016-9552-7              0   \n",
       "...                               ...            ...   \n",
       "2372484      10.1093/EURHEARTJ/EHS282              3   \n",
       "2372485      10.1093/EURHEARTJ/EHS284              0   \n",
       "2372486     10.1038/s41562-017-0189-z              0   \n",
       "2372487         10.31234/osf.io/39ugj              2   \n",
       "2372488    10.15200/WINN.140865.54468              0   \n",
       "\n",
       "                             fields  \\\n",
       "0        ['Biology', 'Engineering']   \n",
       "1        ['Engineering', 'Biology']   \n",
       "2                    ['Psychology']   \n",
       "3              ['Computer Science']   \n",
       "4              ['Computer Science']   \n",
       "...                             ...   \n",
       "2372484                          []   \n",
       "2372485                          []   \n",
       "2372486               ['Economics']   \n",
       "2372487                ['Medicine']   \n",
       "2372488                     ['Art']   \n",
       "\n",
       "                                                 authorIds  \n",
       "0        ['7635770', '49271298', '2773799', '5179706', ...  \n",
       "1        ['2661384', '8646979', '144778908', '51935399'...  \n",
       "2         ['1763293', '145479337', '2773799', '143851134']  \n",
       "3         ['1763293', '145479337', '2773799', '143851134']  \n",
       "4        ['145479337', '2221493', '35063515', '1763293'...  \n",
       "...                                                    ...  \n",
       "2372484  ['1397812736', '1399678807', '143698504', '205...  \n",
       "2372485  ['2960501', '2509309', '1702163', '1406005831'...  \n",
       "2372486  [None, '39201543', '2205017432', '2862527', '2...  \n",
       "2372487  ['2505272', '49115533', '103097236', '20809249...  \n",
       "2372488  ['3149509', '1401864541', '2620049', '13900229...  \n",
       "\n",
       "[2372489 rows x 7 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_data = pd.read_csv(\"paper_data.csv\")\n",
    "paper_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper dataframe has 2372489 entries, and hasn't been cleaned either for duplicates or NaN entries. These papers stem from the 39427 authors listed in the author dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Law of large numbers.\n",
    "\n",
    "As we have discussed in the lecture, one impact of heavy tails is that sample averages can be poor estimators of the underlying mean of the distribution.\n",
    "To understand this point better, recall [the Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers).  Consider a sample of IID variables $ X_1, \\ldots, X_n $ from the same distribution $ F $ with finite expected value $ \\mathbb E |X_i| =  \\int x F(dx)  = \\mu $.\n",
    "\n",
    "According to the law, the mean of the sample $ \\bar X_n := \\frac{1}{n} \\sum_{i=1}^n X_i $ satisfies\n",
    "<a id='equation-lln-as2'></a>\n",
    "$$\n",
    "\\bar X_n \\to \\mu \\text{ as } n \\to \\infty \n",
    "$$\n",
    "\n",
    "This basically tell us that if we have a large enough sample, the sample mean will converge to the population mean. \n",
    "\n",
    "The condition that $ \\mathbb E | X_i | $ is finite holds in most cases but can fail if the distribution $ F $ is very heavy tailed. Further, even when $ \\mathbb E | X_i | $ is finite, the variance of a heavy tailed distribution can be so large that the sample mean will converge very slowly to the population mean. We will look into this in the following exercise. \n",
    "\n",
    "\n",
    "> 1. Sample __N=10,000__ data points from a [Gaussian Distribution](https://en.wikipedia.org/wiki/Normal_distribution) with parameters $\\mu = 0 $ and $\\sigma = 4$, using the [`np.random.standard_normal()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html) function. Store your data in a numpy array $\\mathbf{X}$. \n",
    "> 2. Create a figure. \n",
    ">    - Plot the distribution of the data in $\\mathbf{X}$. \n",
    "> 3. Compute the cumulative average of $\\mathbf{X}$ (you achieve this by computing $average(\\{\\mathbf{X}[0],..., \\mathbf{X}[i-1]\\})$ for each index $i \\in [1, ..., N+1]$  ). Store the result in an array. \n",
    "> 4. In a similar way, compute the cumulative standard error of $\\mathbf{X}$. __Note__: the standard error of a sample is defined as $ \\sigma_{M} = \\frac{\\sigma}{\\sqrt(n)} $, where $\\sigma$ is the sample standard deviation and $n$ is the sample size. Store the result in an array. \n",
    "> 5. Compute the values of the distribution mean and median using the formulas you can find on the [Wikipedia page of the Gaussian Distribution](https://en.wikipedia.org/wiki/Normal_distribution)  \n",
    "> 6. Create a figure. \n",
    ">     - Plot the cumulative average computed in point 3. as a line plot (where the x-axis represent the size of the sample considered, and the y-axis is the average).\n",
    ">     - Add errorbars to each point in the graph with width equal to the standard error of the mean (the one you computed in point 4). \n",
    ">     - Add a horizontal line corresponding to the distribution mean (the one you found in point 5).\n",
    "> 7.  Compute the cumulative median of $\\mathbf{X}$ (you achieve this by computing $median(\\{\\mathbf{X}[0],..., \\mathbf{X}[i-1]\\})$ for each index $i \\in [1, ..., N+1]$). Store the result in an array. \n",
    "> 8. Create a figure. \n",
    ">    - Plot the cumulative median computed in point 7. as a line plot (where the x-axis represent the size of the sample considered, and the y-axis is the average).\n",
    ">    - Add a horizontal line corresponding to the distribution median (the one you found in point 5).\n",
    ">    - _Optional:_ Add errorbars to your median line graph, with width equal to the standard error of the median. You can compute the standard error of the median [via bootstrapping](https://online.stat.psu.edu/stat500/book/export/html/619). \n",
    "> 9. Now sample __N = 10,000__ data points from a [Pareto Distribution](https://en.wikipedia.org/wiki/Pareto_distribution) with parameters $x_m=1$ and $\\alpha=0.5$ using the [`np.random.pareto()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.pareto.html) function, and store it in a numpy array. (_Optional:_ Write yourself the function to sample from a Pareto distribution using the [_Inverse Transform Sampling method_](https://en.wikipedia.org/wiki/Inverse_transform_sampling))\n",
    "> 10. Repeat points 2 to 8 for the Pareto Distribution sample computed in point 9. \n",
    "> 11. Now sample __N = 10,000__ data points from a [Lognormal Distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) with parameters $\\mu=0$ and $\\sigma=4$ using the [`np.random.standard_normal()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html) function, and store it in a numpy array. \n",
    "> 12. Repeat points 2 to 8 for the Lognormal Distribution sample computed in point 11. \n",
    "> 13. Now, consider the array collecting the citations of papers from 2009 you created in Exercise 3, point 1. First, compute the mean and median number of citations for this population. Then, extract a random sample of __N=10,000__ papers.  \n",
    "> 14. Repeat points 2,3,4,6,7 and 8 above for the paper citation sample prepared in point 13. \n",
    "\n",
    "> Answer the following questions:\n",
    "(__Hint__: I suggest you plot the graphs above multiple times for different random samples, to get a better understanding of what is going on)\n",
    "\n",
    ">    -  Compare the evolution of the cumulative average for the Gaussian, Pareto and LogNormal distribution. What do you observe? Would you expect these results? Why?\n",
    ">    - Compare the cumulative median vs the cumulative average for the three distributions. What do you observe? Can you draw any conclusions regarding which statistics (the mean or the median) is more usfeul in the different cases? \n",
    ">    - Consider the plots you made using the citation count data in point 14. What do you observe? What are the implications? \n",
    ">    - What do you think are the main take-home message of this exercise? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "39ec2c5dc35b285f53b68302ee57033e6dc35325dc3fd8d680c0ffb4018a1e0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
